{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Sep 11 21:15:20 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.76                 Driver Version: 560.76         CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090      WDDM  |   00000000:01:00.0  On |                  Off |\n",
      "|  0%   43C    P8             20W /  450W |   17259MiB /  24564MiB |      7%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      3656    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
      "|    0   N/A  N/A      8364    C+G   C:\\Windows\\explorer.exe                     N/A      |\n",
      "|    0   N/A  N/A      8840    C+G   ...oogle\\Chrome\\Application\\chrome.exe      N/A      |\n",
      "|    0   N/A  N/A      9352    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe      N/A      |\n",
      "|    0   N/A  N/A      9424    C+G   ...2txyewy\\StartMenuExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     11520    C+G   ...t.LockApp_cw5n1h2txyewy\\LockApp.exe      N/A      |\n",
      "|    0   N/A  N/A     12960      C   ...3.1-8B-Instruct\\Llamaenv\\python.exe      N/A      |\n",
      "|    0   N/A  N/A     13020    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     13696    C+G   ...crosoft\\Edge\\Application\\msedge.exe      N/A      |\n",
      "|    0   N/A  N/A     14444    C+G   ...inaries\\Win64\\EpicGamesLauncher.exe      N/A      |\n",
      "|    0   N/A  N/A     14960    C+G   ...ne\\Binaries\\Win64\\EpicWebHelper.exe      N/A      |\n",
      "|    0   N/A  N/A     16940    C+G   ...Programs\\Microsoft VS Code\\Code.exe      N/A      |\n",
      "|    0   N/A  N/A     19472    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     19804    C+G   ...on\\128.0.2739.67\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A     20288    C+G   ...Brave-Browser\\Application\\brave.exe      N/A      |\n",
      "|    0   N/A  N/A     21176    C+G   ...siveControlPanel\\SystemSettings.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA version: 12.4\n",
      "Number of CUDA devices: 1\n",
      "Current CUDA device: 0\n",
      "Name of CUDA device: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"Number of CUDA devices: {torch.cuda.device_count()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "    print(f\"Name of CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cente\\Desktop\\ilmaan project\\llama3.1fine\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "# import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.4.0 requires torch==2.4.0, but you have torch 2.3.1 which is incompatible.\n",
      "torchvision 0.19.0 requires torch==2.4.0, but you have torch 2.3.1 which is incompatible.\n",
      "xformers 0.0.27.post2 requires torch==2.4.0, but you have torch 2.3.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# !pip install -Uqqq pip --progress-bar off\n",
    "# !pip install -qqq torch==2.3.1 --progress-bar off\n",
    "# !pip install -qqq transformers==4.41.2 --progress-bar off\n",
    "# !pip install -qqq datasets==2.20.0 --progress-bar off\n",
    "# !pip install -qqq accelerate==0.31.0 --progress-bar off\n",
    "# !pip install -qqq bitsandbytes==0.43.1 --progress-bar off\n",
    "# !pip install -qqq peft==0.11.1 --progress-bar off\n",
    "# !pip install -qqq trl==0.9.4 --progress-bar off\n",
    "# !pip install -qqq colored==2.2.4 --progress-bar off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -qqq transformers==4.41.2 --progress-bar off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from matplotlib) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from matplotlib) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: seaborn in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from seaborn) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from seaborn) (2.2.2)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from seaborn) (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from pandas>=1.2->seaborn) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from scikit-learn) (1.14.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from scikit-learn) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "# !pip install matplotlib\n",
    "# !pip install seaborn\n",
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: colored in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (2.2.4)\n"
     ]
    }
   ],
   "source": [
    "# !pip install transformers==4.43.1\n",
    "# !pip install vllm==0.5.3.post1\n",
    "!pip install colored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: peft in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (0.12.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from peft) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from peft) (24.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from peft) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from peft) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from peft) (2.4.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from peft) (4.45.0.dev0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from peft) (4.66.4)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from peft) (0.34.0.dev0)\n",
      "Requirement already satisfied: safetensors in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from peft) (0.4.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from peft) (0.24.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\cente\\appdata\\roaming\\python\\python312\\site-packages (from huggingface-hub>=0.17.0->peft) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from huggingface-hub>=0.17.0->peft) (2024.3.1)\n",
      "Requirement already satisfied: requests in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from huggingface-hub>=0.17.0->peft) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from torch>=1.13.0->peft) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from torch>=1.13.0->peft) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from torch>=1.13.0->peft) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from torch>=1.13.0->peft) (72.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from tqdm->peft) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from transformers->peft) (2024.7.24)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from transformers->peft) (0.19.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from requests->huggingface-hub>=0.17.0->peft) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.7.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: trl in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (0.10.1)\n",
      "Requirement already satisfied: torch>=1.4.0 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from trl) (2.4.0)\n",
      "Requirement already satisfied: transformers>=4.31.0 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from trl) (4.45.0.dev0)\n",
      "Requirement already satisfied: numpy>=1.18.2 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from trl) (1.26.4)\n",
      "Requirement already satisfied: accelerate in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from trl) (0.34.0.dev0)\n",
      "Requirement already satisfied: datasets in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from trl) (2.19.1)\n",
      "Requirement already satisfied: tyro>=0.5.11 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from trl) (0.8.6)\n",
      "Requirement already satisfied: filelock in c:\\users\\cente\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.4.0->trl) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from torch>=1.4.0->trl) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from torch>=1.4.0->trl) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from torch>=1.4.0->trl) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from torch>=1.4.0->trl) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from torch>=1.4.0->trl) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from torch>=1.4.0->trl) (72.1.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from transformers>=4.31.0->trl) (0.24.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from transformers>=4.31.0->trl) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from transformers>=4.31.0->trl) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from transformers>=4.31.0->trl) (2024.7.24)\n",
      "Requirement already satisfied: requests in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from transformers>=4.31.0->trl) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from transformers>=4.31.0->trl) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from transformers>=4.31.0->trl) (0.4.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from transformers>=4.31.0->trl) (4.66.4)\n",
      "Requirement already satisfied: docstring-parser>=0.16 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from tyro>=0.5.11->trl) (0.16)\n",
      "Requirement already satisfied: rich>=11.1.0 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from tyro>=0.5.11->trl) (13.7.1)\n",
      "Requirement already satisfied: shtab>=1.5.6 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from tyro>=0.5.11->trl) (1.7.1)\n",
      "Requirement already satisfied: colorama>=0.4.0 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from tyro>=0.5.11->trl) (0.4.6)\n",
      "Requirement already satisfied: psutil in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from accelerate->trl) (5.9.0)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from datasets->trl) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from datasets->trl) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from datasets->trl) (2.2.2)\n",
      "Requirement already satisfied: xxhash in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from datasets->trl) (2.0.2)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from datasets->trl) (0.70.15)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from datasets->trl) (3.9.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from aiohttp->datasets->trl) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from aiohttp->datasets->trl) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from aiohttp->datasets->trl) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from aiohttp->datasets->trl) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from aiohttp->datasets->trl) (1.9.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from requests->transformers>=4.31.0->trl) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from requests->transformers>=4.31.0->trl) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from requests->transformers>=4.31.0->trl) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from requests->transformers>=4.31.0->trl) (2024.7.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.18.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from jinja2->torch>=1.4.0->trl) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from pandas->datasets->trl) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from pandas->datasets->trl) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from pandas->datasets->trl) (2023.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from sympy->torch>=1.4.0->trl) (1.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets->trl) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# !pip install peft\n",
    "# !pip install trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cente\\Desktop\\ilmaan project\\llama3.1fine\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from textwrap import dedent\n",
    "from typing import Dict, List\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from colored import Back, Fore, Style\n",
    "from datasets import Dataset, load_dataset\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModel,\n",
    "    TaskType,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline,\n",
    ")\n",
    "from trl import DataCollatorForCompletionOnlyLM, SFTConfig, SFTTrainer\n",
    "\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format='retina'\n",
    "\n",
    "# COLORS = [\"#bae1ff\", \"#ffb3ba\", \"#ffdfba\", \"#ffffba\", \"#baffc9\"]\n",
    "\n",
    "# sns.set(style=\"whitegrid\", palette=\"muted\", font_scale=1.2)\n",
    "# sns.set_palette(sns.color_palette(COLORS))\n",
    "\n",
    "# cmap = colors.LinearSegmentedColormap.from_list(\"custom_cmap\", COLORS[:2])\n",
    "\n",
    "# MY_STYLE = {\n",
    "#     \"figure.facecolor\": \"black\",\n",
    "#     \"axes.facecolor\": \"black\",\n",
    "#     \"axes.edgecolor\": \"white\",\n",
    "#     \"axes.labelcolor\": \"white\",\n",
    "#     \"axes.linewidth\": 0.5,\n",
    "#     \"text.color\": \"white\",\n",
    "#     \"xtick.color\": \"white\",\n",
    "#     \"ytick.color\": \"white\",\n",
    "#     \"grid.color\": \"gray\",\n",
    "#     \"grid.linestyle\": \"--\",\n",
    "#     \"grid.linewidth\": 0.5,\n",
    "#     \"axes.grid\": True,\n",
    "#     \"xtick.labelsize\": \"medium\",\n",
    "#     \"ytick.labelsize\": \"medium\",\n",
    "#     \"axes.titlesize\": \"large\",\n",
    "#     \"axes.labelsize\": \"large\",\n",
    "#     \"lines.color\": COLORS[0],\n",
    "#     \"patch.edgecolor\": \"white\",\n",
    "# }\n",
    "\n",
    "# mpl.rcParams.update(MY_STYLE)\n",
    "\n",
    "# SEED = 42\n",
    "\n",
    "\n",
    "# def seed_everything(seed: int):\n",
    "#     random.seed(seed)\n",
    "#     np.random.seed(seed)\n",
    "#     torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "# seed_everything(SEED)\n",
    "# PAD_TOKEN = \"<|pad|>\"\n",
    "# MODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "# NEW_MODEL = \"Llama-3-8B-Instruct-Finance-RAG\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MODEL_NAME' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m quantization_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(\n\u001b[0;32m      2\u001b[0m     load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, bnb_4bit_quant_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnf4\u001b[39m\u001b[38;5;124m\"\u001b[39m, bnb_4bit_compute_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16\n\u001b[0;32m      3\u001b[0m )\n\u001b[1;32m----> 5\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[43mMODEL_NAME\u001b[49m, use_fast\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39madd_special_tokens({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpad_token\u001b[39m\u001b[38;5;124m\"\u001b[39m: PAD_TOKEN})\n\u001b[0;32m      7\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpadding_side \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MODEL_NAME' is not defined"
     ]
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "tokenizer.add_special_tokens({\"pad_token\": PAD_TOKEN})\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=quantization_config,\n",
    "    #     attn_implementation=\"flash_attention_2\",\n",
    "    #     attn_implementation=\"sdpa\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.resize_token_embeddings(len(tokenizer), pad_to_multiple_of=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"c:\\\\Users\\\\cente\\\\Desktop\\\\ilmaan project\\\\llama3.1\\\\Meta-Llama-3.1-8B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (2.3.1)\n",
      "Collecting torch\n",
      "  Using cached torch-2.4.1-cp312-cp312-win_amd64.whl.metadata (27 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\cente\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from torch) (72.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Using cached torch-2.4.1-cp312-cp312-win_amd64.whl (199.4 MB)\n",
      "Installing collected packages: torch\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.3.1\n",
      "    Uninstalling torch-2.3.1:\n",
      "      Successfully uninstalled torch-2.3.1\n",
      "Successfully installed torch-2.4.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\cente\\Desktop\\ilmaan project\\llama3.1fine\\Lib\\site-packages\\~~rch'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.4.0 requires torch==2.4.0, but you have torch 2.4.1 which is incompatible.\n",
      "torchvision 0.19.0 requires torch==2.4.0, but you have torch 2.4.1 which is incompatible.\n",
      "xformers 0.0.27.post2 requires torch==2.4.0, but you have torch 2.4.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "# !pip install torch --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (4.41.2)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\cente\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from transformers) (0.24.5)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from transformers) (2024.7.24)\n",
      "Requirement already satisfied: requests in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from transformers) (0.4.4)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from requests->transformers) (2024.7.4)\n",
      "Using cached transformers-4.44.2-py3-none-any.whl (9.5 MB)\n",
      "Installing collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.41.2\n",
      "    Uninstalling transformers-4.41.2:\n",
      "      Successfully uninstalled transformers-4.41.2\n",
      "Successfully installed transformers-4.44.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install transformers --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.62s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(128264, 4096)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaForCausalLM\n",
    "from transformers import BitsAndBytesConfig  # Add this import\n",
    "\n",
    "SEED = 42\n",
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "seed_everything(SEED)\n",
    "\n",
    "PAD_TOKEN = \"<|pad|>\"\n",
    "MODEL_NAME = \"c:\\\\Users\\\\cente\\\\Desktop\\\\ilmaan project\\\\llama3.1\\\\Meta-Llama-3.1-8B-Instruct\"\n",
    "NEW_MODEL = \"Llama-3-8B-Instruct-VULSCANNER-RAG\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, \n",
    "    bnb_4bit_quant_type=\"nf4\", \n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True, trust_remote_code=True)\n",
    "tokenizer.add_special_tokens({\"pad_token\": PAD_TOKEN})\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    # quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "model.resize_token_embeddings(len(tokenizer), pad_to_multiple_of=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaConfig {\n",
       "  \"_name_or_path\": \"c:\\\\Users\\\\cente\\\\Desktop\\\\ilmaan project\\\\llama3.1\\\\Meta-Llama-3.1-8B-Instruct\",\n",
       "  \"architectures\": [\n",
       "    \"LlamaForCausalLM\"\n",
       "  ],\n",
       "  \"attention_bias\": false,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 128000,\n",
       "  \"eos_token_id\": [\n",
       "    128001,\n",
       "    128008,\n",
       "    128009\n",
       "  ],\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 4096,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 14336,\n",
       "  \"max_position_embeddings\": 131072,\n",
       "  \"mlp_bias\": false,\n",
       "  \"model_type\": \"llama\",\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 32,\n",
       "  \"num_key_value_heads\": 8,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_scaling\": {\n",
       "    \"factor\": 8.0,\n",
       "    \"high_freq_factor\": 4.0,\n",
       "    \"low_freq_factor\": 1.0,\n",
       "    \"original_max_position_embeddings\": 8192,\n",
       "    \"rope_type\": \"llama3\"\n",
       "  },\n",
       "  \"rope_theta\": 500000.0,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"bfloat16\",\n",
       "  \"transformers_version\": \"4.44.2\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 128264\n",
       "}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<|begin_of_text|>', 128000)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.bos_token, tokenizer.bos_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<|eot_id|>', 128009)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token, tokenizer.eos_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<|pad|>', 128256)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token, tokenizer.pad_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128256"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(PAD_TOKEN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=128,\n",
    "    return_full_text=False,\n",
    ")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_prompt():\n",
    "    prompt = dedent(\n",
    "        f\"\"\"\n",
    "\n",
    "        import os\n",
    "\n",
    "class RobotAuthenticator:\n",
    "    def __init__(self):\n",
    "        self.auth_token = os.getenv(\"\"ROBOT_AUTH_TOKEN\"\")\n",
    "\n",
    "    def authenticate(self):\n",
    "        print(f\"Authenticating robot with token: self.auth_token\")\n",
    "        # Authentication logic here...\n",
    "    Is the above code Secure or Insecure. If it is insecure give the CWE-id followed by the resaon why it is insecure, followed by secure version of the code.\n",
    "    \"\"\"\n",
    "    )\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Use only the information to answer the question\",\n",
    "        },\n",
    "        {\"role\": \"tester\", \"content\": prompt},\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "Use only the information to answer the question<|eot_id|><|start_header_id|>tester<|end_header_id|>\n",
      "\n",
      "import os\n",
      "\n",
      "class RobotAuthenticator:\n",
      "    def __init__(self):\n",
      "        self.auth_token = os.getenv(\"\"ROBOT_AUTH_TOKEN\"\")\n",
      "\n",
      "    def authenticate(self):\n",
      "        print(f\"Authenticating robot with token: self.auth_token\")\n",
      "        # Authentication logic here...\n",
      "    Is the above code Secure or Insecure. If it is insecure give the CWE-id followed by the resaon why it is insecure, followed by secure version of the code.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# row = dataset[\"test\"][0]\n",
    "prompt = create_test_prompt()\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "answer:     \n",
      "prediction:\n",
      "\n",
      " The above code is Insecure. \n",
      "\n",
      "CWE-id: CWE-798: Use of Hard-coded Credentials.\n",
      "\n",
      "Reason: The authentication token is hardcoded directly into the code using the `os.getenv()` function, which is not secure. If the token is exposed in the environment variable, it could be accessed by unauthorized parties, potentially leading to unauthorized access to the robot.\n",
      "\n",
      "Secure Version of the Code:\n",
      "\n",
      "```python\n",
      "import os\n",
      "import dotenv\n",
      "\n",
      "class RobotAuthenticator:\n",
      "    def __init__(self):\n",
      "        dotenv.load_dotenv()\n",
      "        self.auth_token = os.getenv(\"ROBOT_AUTH_TOKEN\")\n",
      "\n",
      "    def authenticate(self):\n",
      "        print(f\"\n",
      "\n",
      "CPU times: total: 9min 18s\n",
      "Wall time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "outputs = pipe(prompt)\n",
    "response = f\"\"\"\n",
    "answer:     \n",
    "prediction:\\n\\n {outputs[0][\"generated_text\"]}\n",
    "\"\"\"\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load your CSV dataset\n",
    "df = pd.read_csv('C:\\\\Users\\\\cente\\\\Desktop\\\\vulscanner\\\\New folder\\\\VulScannerAi\\\\vulscanner\\\\scanner\\\\cwe1_python_dataset (1).csv')  # Replace with the path to your CSV file\n",
    "# VulScannerAi\\vulscanner\\scanner\\cwe1_python_dataset (1).csv\n",
    "# Step 2: Format the CSV data\n",
    "# Assuming the CSV has columns: CWE_ID, Insecure_Code, Incoder_Code, Copilot_Code, Prompt\n",
    "# Modify column names as per your dataset structure\n",
    "df = df[['CWE_ID', 'Insecure_Code', 'Incoder_Code', 'Copilot_Code', 'Prompt']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Define a function to format each example into the required prompt/response format\n",
    "def format_example(row: dict):\n",
    "    prompt = dedent(\n",
    "        f\"\"\"\n",
    "    {row[\"Prompt\"]}\n",
    "\n",
    "    Insecure Code:\n",
    "\n",
    "    ```\n",
    "    {row[\"Insecure_Code\"]}\n",
    "    ```\n",
    "    \"\"\"\n",
    "    )\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Fix the security issues in the given code\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "        {\"role\": \"assistant\", \"content\": row[\"Copilot_Code\"]},  # Replace with the target code (e.g., the secure code)\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(messages, tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import dedent\n",
    "\n",
    "def format_example(row: dict):\n",
    "    # Check if the code is secure or insecure based on the dataset (assuming 'is_secure' column indicates this)\n",
    "    is_insecure = row.get(\"is_secure\", \"insecure\") == \"insecure\"  # Assuming 'is_secure' indicates if the code is secure\n",
    "    \n",
    "    if is_insecure:\n",
    "        # If the code is insecure, format the message with CWE-ID, reason, and secure version\n",
    "        reason = \"Generate reason using the model\"  # Replace with actual reason generation if necessary\n",
    "        \n",
    "        prompt = dedent(\n",
    "            f\"\"\"\n",
    "            The above code is Insecure.\n",
    "\n",
    "            **CWE-id:** {row[\"CWE_ID\"]}\n",
    "\n",
    "            **Reason:** {reason}\n",
    "\n",
    "            **Secure Version:**\n",
    "\n",
    "            ```python\n",
    "            {row[\"Copilot_Code\"]}\n",
    "            ```\n",
    "            \"\"\"\n",
    "        )\n",
    "    else:\n",
    "        # If the code is secure, return the secure message\n",
    "        prompt = \"The above code is secure.\"\n",
    "\n",
    "    # Create the conversation messages\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Determine if the given code is secure or insecure, and provide the secure version if insecure.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": row[\"Prompt\"]},  # This prompt will include insecure code details\n",
    "        {\"role\": \"assistant\", \"content\": prompt},  # Response from the assistant\n",
    "    ]\n",
    "    \n",
    "    return tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Apply formatting to each row of the dataset\n",
    "df[\"text\"] = df.apply(format_example, axis=1)\n",
    "\n",
    "# Step 5: Count tokens to ensure they fit the model's token limit\n",
    "def count_tokens(row: dict) -> int:\n",
    "    return len(\n",
    "        tokenizer(\n",
    "            row[\"text\"],\n",
    "            add_special_tokens=True,\n",
    "            return_attention_mask=False,\n",
    "        )[\"input_ids\"]\n",
    "    )\n",
    "\n",
    "df[\"token_count\"] = df.apply(count_tokens, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Filter out rows with token counts > 512 (adjust as per your model's token limit)\n",
    "df = df[df[\"token_count\"] < 512]\n",
    "\n",
    "# Step 7: Split into train, validation, and test sets\n",
    "train, temp = train_test_split(df, test_size=0.2)\n",
    "val, test = train_test_split(temp, test_size=0.2)\n",
    "\n",
    "# Optional: Sample smaller datasets if needed\n",
    "# train = train.sample(n=4000)\n",
    "# val = val.sample(n=500)\n",
    "# test = test.sample(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 8: Save the splits as JSON files for loading later\n",
    "train.to_json(\"train.json\", orient=\"records\", lines=True)\n",
    "val.to_json(\"val.json\", orient=\"records\", lines=True)\n",
    "test.to_json(\"test.json\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 222 examples [00:00, 13067.65 examples/s]\n",
      "Generating validation split: 44 examples [00:00, 8345.74 examples/s]\n",
      "Generating test split: 12 examples [00:00, 2965.57 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Step 9: Load the datasets for training\n",
    "dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\"train\": \"train.json\", \"validation\": \"val.json\", \"test\": \"test.json\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['CWE_ID', 'Insecure_Code', 'Incoder_Code', 'Copilot_Code', 'Prompt', 'text', 'token_count'],\n",
      "        num_rows: 222\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['CWE_ID', 'Insecure_Code', 'Incoder_Code', 'Copilot_Code', 'Prompt', 'text', 'token_count'],\n",
      "        num_rows: 44\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['CWE_ID', 'Insecure_Code', 'Incoder_Code', 'Copilot_Code', 'Prompt', 'text', 'token_count'],\n",
      "        num_rows: 12\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Step 10: View the dataset\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 11: Visualize token distribution\n",
    "plt.hist(df[\"token_count\"], weights=np.ones(len(df[\"token_count\"])) / len(df[\"token_count\"]))\n",
    "plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
    "plt.xlabel(\"Tokens\")\n",
    "plt.ylabel(\"Percentage\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "Fix the security issues in the given code<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Get the filename from the request argument,\n",
      "    validate the filename,\n",
      "    and return the data of the file.\n",
      "\n",
      "    Insecure Code:\n",
      "\n",
      "    ```\n",
      "    from flask import Flask, request\n",
      "app = Flask(__name__)\n",
      "\n",
      "@app.route(\"/filedata\")\n",
      "def getFileData():\n",
      "\n",
      "    filename = request.args.get('filename')\n",
      "    data = open(filename, 'rb').read()\n",
      "    return data\n",
      "    ```<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "from urllib3 import HTTPResponse\n",
      "\n",
      "def getFileData(request):\n",
      "    \n",
      "    file_name = request.args.get('file_name')\n",
      "    with open(file_name, 'rb') as file:\n",
      "        data = file.read()\n",
      "    return HTTPResponse(data)<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "# Step 12: Tokenize and fine-tune the model as per your previous process\n",
    "\n",
    "# Example tokenized dataset structure:\n",
    "print(dataset[\"test\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Original Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_prompt(row:dict):\n",
    "    prompt = dedent(\n",
    "        f\"\"\"\n",
    "    {row[\"Prompt\"]}\n",
    "\n",
    "    Insecure Code:\n",
    "\n",
    "    ```\n",
    "    {row[\"Insecure_Code\"]}\n",
    "    ```\n",
    "    \"\"\"\n",
    "    )\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Fix the security issues in the given code\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "        {\"role\": \"assistant\", \"content\": row[\"Copilot_Code\"]},  # Replace with the target code (e.g., the secure code)\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "# Step 3: Define a function to format each example into the required prompt/response format\n",
    "def format_example(row: dict):\n",
    "    prompt = dedent(\n",
    "        f\"\"\"\n",
    "    {row[\"Prompt\"]}\n",
    "\n",
    "    Insecure Code:\n",
    "\n",
    "    ```\n",
    "    {row[\"Insecure_Code\"]}\n",
    "    ```\n",
    "    \"\"\"\n",
    "    )\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Fix the security issues in the given code\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "        {\"role\": \"assistant\", \"content\": row[\"Copilot_Code\"]},  # Replace with the target code (e.g., the secure code)\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(messages, tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>Tester<|end_header_id|>\n",
      "\n",
      "Insecure Code:\n",
      "\n",
      "    **CWE-id:** CWE-416\n",
      "\n",
      "    **Reason:** Reason to be generated by the model based on the security flaw in the code.\n",
      "\n",
      "    **Secure Version:**\n",
      "\n",
      "    ```python\n",
      "    class SensorDataProcessor:\n",
      "def __init__(self):\n",
      "    self.data = [1, 2, 3, 4, 5]\n",
      "\n",
      "def process_data(self):\n",
      "    print(\"Processing sensor data...\")\n",
      "\n",
      "def cleanup(self):\n",
      "    del self.data\n",
      "    print(\"Sensor data cleaned up.\")\n",
      "\n",
      "def process_and_cleanup(self):\n",
      "    self.process_data()\n",
      "    self.cleanup()\n",
      "    # No further use of `self.data` here to avoid use after free\n",
      "    ```\n",
      "\n",
      "    Now determine if the above code is secure or insecure. \n",
      "    If it is insecure, provide the CWE-ID followed by the reason why it is insecure and then provide a secure version of the code.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "row = dataset[\"test\"][1]\n",
    "prompt = create_test_prompt(row)\n",
    "print(prompt)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "answer:     \n",
      "prediction: The code provided is insecure.\n",
      "\n",
      "**CWE-id:** CWE-416\n",
      "\n",
      "**Reason:** The code is insecure because it uses an object after it has been deleted. In the `cleanup` method, the `self.data` attribute is deleted using `del self.data`. However, in the `process_and_cleanup` method, the `process_data` method is called before `cleanup`, which still tries to access the `self.data` attribute. This is a classic example of a \"Use After Free\" vulnerability, where memory is accessed after it has been freed.\n",
      "\n",
      "**Secure Version:**\n",
      "\n",
      "```python\n",
      "class SensorDataProcessor:\n",
      "    def\n",
      "\n",
      "CPU times: total: 11min 43s\n",
      "Wall time: 1min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "outputs = pipe(prompt)\n",
    "response = f\"\"\"\n",
    "answer:     \n",
    "prediction: {outputs[0][\"generated_text\"]}\n",
    "\"\"\"\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import dedent\n",
    "\n",
    "def create_test_prompt(data_row):\n",
    "    # Generate reason using a model or predefined text (Replace this with model generation if needed)\n",
    "    reason = \"Generate reason using the model\"  # Placeholder, replace with actual model output\n",
    "\n",
    "    prompt = dedent(\n",
    "        f\"\"\"\n",
    "        The above code is Insecure.\n",
    "\n",
    "        **CWE-id:** {data_row[\"CWE_ID\"]}\n",
    "\n",
    "        **Reason:** {reason}\n",
    "\n",
    "        **Secure Version:**\n",
    "\n",
    "        ```python\n",
    "        {data_row[\"Copilot_Code\"]}\n",
    "        ```\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\":\"Tester\",\"content\":prompt+'''\n",
    "            Is the above code Secure or Insecure. If it is insecure give the CWE-id followed by the resaon why it is insecure, followed by secure version of the code.\n",
    "            ''',}\n",
    "        # {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    \n",
    "    return tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just For Testing\n",
    "from textwrap import dedent\n",
    "\n",
    "def create_test_prompt(data_row):\n",
    "    # Placeholder for generating the reason using a model, if needed\n",
    "    reason = \"Reason to be generated by the model based on the security flaw in the code.\"  # This can be replaced with dynamic generation\n",
    "\n",
    "    # Construct the prompt\n",
    "    prompt = dedent(\n",
    "        f\"\"\"\n",
    "        Insecure Code:\n",
    "\n",
    "        **CWE-id:** {data_row[\"CWE_ID\"]}\n",
    "\n",
    "        **Reason:** {reason}\n",
    "\n",
    "        **Secure Version:**\n",
    "\n",
    "        ```python\n",
    "        {data_row[\"Copilot_Code\"]}\n",
    "        ```\n",
    "\n",
    "        Now determine if the above code is secure or insecure. \n",
    "        If it is insecure, provide the CWE-ID followed by the reason why it is insecure and then provide a secure version of the code.\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    # Define messages in the chat format\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"Tester\",\n",
    "            \"content\": prompt  # Full prompt that asks the model to classify the code and provide detailed info if insecure\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    return tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [14:20<00:00, 71.72s/it]\n"
     ]
    }
   ],
   "source": [
    "rows = []\n",
    "for row in tqdm(dataset[\"test\"]):\n",
    "    prompt = create_test_prompt(row)\n",
    "    outputs = pipe(prompt)\n",
    "    rows.append(\n",
    "        {\n",
    "            \"CWE_ID\": row[\"CWE_ID\"],\n",
    "            \"Insecure_Code\": row[\"Insecure_Code\"],\n",
    "            \"Incoder_Code\": row[\"Incoder_Code\"],\n",
    "            \"Copilot_Code\": row[\"Copilot_Code\"],\n",
    "            \"Prompt\": row[\"Prompt\"],\n",
    "            \"Generated_Prompt\": prompt,\n",
    "            \"Model_Output\": outputs[0][\"generated_text\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "predictions_df = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train on Completions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_template = \"<|end_header_id|>\"\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n",
    "\n",
    "examples = [dataset[\"train\"][0][\"text\"]]\n",
    "encodings = [tokenizer(e) for e in examples]\n",
    "\n",
    "dataloader = DataLoader(encodings, collate_fn=collator, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'labels'])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "batch = next(iter(dataloader))\n",
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "           -100,   -100,   -100,   -100,   -100,   -100,    271,    791,   3485,\n",
       "           2082,    374,    763,  26189,    382,    310,   3146,     34,  12739,\n",
       "          13193,  68063,  51960,     12,  26519,    271,    310,   3146,  26197,\n",
       "          68063,  20400,   2944,   1701,    279,   1646,    271,    310,   3146,\n",
       "          50913,   6207,     25,  57277,    310,  55375,  12958,    198,    310,\n",
       "            505,   8426,   7109,   1179,  18040,    198,   1527,   8426,  43368,\n",
       "           1179,   6559,    198,    755,   1920,   8052,   4980,   7887,    262,\n",
       "           3636,    284,   1715,  17866,    681,   3918,   4532,    262,    422,\n",
       "           3636,    624,    364,   3918,   3730,    286,    471,   6559,   3478,\n",
       "           3758,   1329,    262,    775,    512,    286,    471,  18040,    493,\n",
       "           3767,   1329,    310,  55375, 128009]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LoRA Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128264, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128264, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\n",
    "        \"self_attn.q_proj\",\n",
    "        \"self_attn.k_proj\",\n",
    "        \"self_attn.v_proj\",\n",
    "        \"self_attn.o_proj\",\n",
    "        \"mlp.gate_proj\",\n",
    "        \"mlp.up_proj\",\n",
    "        \"mlp.down_proj\",\n",
    "    ],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 83,886,080 || all params: 8,114,212,864 || trainable%: 1.0338\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorboard\n",
      "  Downloading tensorboard-2.17.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting absl-py>=0.4 (from tensorboard)\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting grpcio>=1.48.2 (from tensorboard)\n",
      "  Downloading grpcio-1.66.1-cp312-cp312-win_amd64.whl.metadata (4.0 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard)\n",
      "  Downloading Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: numpy>=1.12.0 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from tensorboard) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from tensorboard) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from tensorboard) (3.20.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from tensorboard) (72.1.0)\n",
      "Requirement already satisfied: six>1.9 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from tensorboard) (1.16.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard)\n",
      "  Downloading werkzeug-3.0.4-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\cente\\desktop\\ilmaan project\\llama3.1fine\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.3)\n",
      "Downloading tensorboard-2.17.1-py3-none-any.whl (5.5 MB)\n",
      "   ---------------------------------------- 0.0/5.5 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.8/5.5 MB 4.2 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 1.6/5.5 MB 4.2 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 2.4/5.5 MB 4.3 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 3.4/5.5 MB 4.2 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 4.2/5.5 MB 4.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 5.0/5.5 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.5/5.5 MB 4.1 MB/s eta 0:00:00\n",
      "Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Downloading grpcio-1.66.1-cp312-cp312-win_amd64.whl (4.3 MB)\n",
      "   ---------------------------------------- 0.0/4.3 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.8/4.3 MB 4.8 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 1.6/4.3 MB 4.4 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 2.4/4.3 MB 4.3 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 3.1/4.3 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  4.2/4.3 MB 4.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.3/4.3 MB 4.2 MB/s eta 0:00:00\n",
      "Downloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Downloading werkzeug-3.0.4-py3-none-any.whl (227 kB)\n",
      "Installing collected packages: werkzeug, tensorboard-data-server, markdown, grpcio, absl-py, tensorboard\n",
      "Successfully installed absl-py-2.1.0 grpcio-1.66.1 markdown-3.7 tensorboard-2.17.1 tensorboard-data-server-0.7.2 werkzeug-3.0.4\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Launching TensorBoard..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1c80317fa3b1799d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1c80317fa3b1799d\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "OUTPUT_DIR = \"experiments\"\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir \"experiments/runs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 222/222 [00:00<00:00, 6501.34 examples/s]\n",
      "Map: 100%|██████████| 44/44 [00:00<00:00, 2932.99 examples/s]\n",
      "c:\\Users\\cente\\Desktop\\ilmaan project\\llama3.1fine\\Lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "sft_config = SFTConfig(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=512,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=0.2,\n",
    "    save_steps=0.2,\n",
    "    logging_steps=10,\n",
    "    learning_rate=1e-4,\n",
    "    fp16=True,  # or bf16=True,\n",
    "    save_strategy=\"steps\",\n",
    "    warmup_ratio=0.1,\n",
    "    save_total_limit=2,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    report_to=\"tensorboard\",\n",
    "    save_safetensors=True,\n",
    "    dataset_kwargs={\n",
    "        \"add_special_tokens\": False,  # We template with special tokens\n",
    "        \"append_concat_token\": False,  # No need to add additional separator token\n",
    "    },\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=sft_config,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/27 [00:00<?, ?it/s]c:\\Users\\cente\\Desktop\\ilmaan project\\llama3.1fine\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:660: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      " 22%|██▏       | 6/27 [15:51<54:34, 155.91s/it]  We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "                                               \n",
      " 22%|██▏       | 6/27 [22:32<54:34, 155.91s/it]c:\\Users\\cente\\Desktop\\ilmaan project\\llama3.1fine\\Lib\\site-packages\\peft\\utils\\save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8993819952011108, 'eval_runtime': 400.8836, 'eval_samples_per_second': 0.11, 'eval_steps_per_second': 0.055, 'epoch': 0.22}\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(NEW_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Trained Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(NEW_MODEL)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer), pad_to_multiple_of=8)\n",
    "model = PeftModel.from_pretrained(model, NEW_MODEL)\n",
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub(NEW_MODEL, tokenizer=tokenizer, max_shard_size=\"5GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.push_to_hub(NEW_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\"train\": \"train.json\", \"validation\": \"val.json\", \"test\": \"test.json\"},\n",
    ")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MODEL_NAME = \"curiousily/Llama-3-8B-Instruct-vulscanner-RAG\"\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, quantization_config=quantization_config, device_map=\"auto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=128,\n",
    "    return_full_text=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = dataset[\"test\"][0]\n",
    "prompt = create_test_prompt(row)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "outputs = pipe(prompt)\n",
    "response = f\"\"\"\n",
    "answer:     {row[\"answer\"]}\n",
    "prediction: {outputs[0][\"generated_text\"]}\n",
    "\"\"\"\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = dataset[\"test\"][1]\n",
    "prompt = create_test_prompt(row)\n",
    "print(prompt)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "outputs = pipe(prompt)\n",
    "response = f\"\"\"\n",
    "answer:     {row[\"answer\"]}\n",
    "prediction: {outputs[0][\"generated_text\"]}\n",
    "\"\"\"\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = dataset[\"test\"][2]\n",
    "prompt = create_test_prompt(row)\n",
    "print(prompt)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "outputs = pipe(prompt)\n",
    "response = f\"\"\"\n",
    "answer:     {row[\"answer\"]}\n",
    "prediction: {outputs[0][\"generated_text\"]}\n",
    "\"\"\"\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictions = []\n",
    "for row in tqdm(dataset[\"test\"]):\n",
    "    outputs = pipe(create_test_prompt(row))\n",
    "    predictions.append(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df[\"trained_prediction\"] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictions_df.to_csv(\"predictions.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = predictions_df.sample(n=20)\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in sample.head(n=10).reset_index().iterrows():\n",
    "    print(f\"{Fore.DARK_VIOLET_1A}{Back.WHITE}Example {i + 1}{Style.reset}\")\n",
    "    response = f\"\"\"\n",
    "{Fore.BLUE}answer:{Style.reset} {row['answer']}\n",
    "\n",
    "{Fore.GREEN}trained:{Style.reset} {row['trained_prediction']}\n",
    "\n",
    "{Fore.DARK_ORANGE}untrained:{Style.reset} {row['untrained_prediction']}\n",
    "\"\"\"\n",
    "    print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
